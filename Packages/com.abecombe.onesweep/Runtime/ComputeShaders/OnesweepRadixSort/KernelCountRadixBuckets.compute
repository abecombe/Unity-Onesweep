// This file includes code adapted from GPUSorting by Thomas Smith
// https://github.com/b0nes164/GPUSorting
// Licensed under the MIT License
// Modified by abecombe, 2025

#pragma kernel CountRadixBuckets

#pragma multi_compile KEY_TYPE_UINT KEY_TYPE_INT KEY_TYPE_FLOAT
#pragma multi_compile SORTING_ORDER_ASCENDING SORTING_ORDER_DESCENDING
#pragma multi_compile USE_DIRECT_DISPATCH USE_INDIRECT_DISPATCH

#include "../RadixCommon/Radix.hlsl"
#include "../RadixCommon/DispatchUtils.hlsl"

#define ITEMS_PER_THREAD (64u)

#define THREADS_PER_GROUP (128u)
#define ITEMS_PER_GROUP (ITEMS_PER_THREAD * THREADS_PER_GROUP)

RWStructuredBuffer<uint> bucket_count_buffer; // size: RADIX_BASE * RADIX_STEP_COUNT
StructuredBuffer<DATA_TYPE> key_in_buffer;

#define GROUP_SHARED_SIZE (RADIX_BASE * 2u)
groupshared uint4 group_shared[GROUP_SHARED_SIZE];

/**
 * \brief Builds a global histogram covering the four 8-bit passes of 32-bit keys in a single dispatch.
 *
 * \note dispatch group size: ceil(sort_item_count / ITEMS_PER_GROUP)
 *
 * This kernel efficiently counts the occurrences of each possible byte value (0-255)
 * at each of the four byte positions (passes) within the input keys.
 * Each thread group processes a segment of the input keys (`key_in_buffer`).
 * For every key, it determines which bucket it falls into for all four 8-bit radix passes
 * simultaneously (corresponding to bits 0-7, 8-15, 16-23, and 24-31 for 32-bit keys).
 *
 * Counts are first aggregated locally within `group_shared` memory (using `uint4` to hold
 * counts for all four passes for a given bucket index, and employing InterlockedAdds).
 * After local aggregation, these per-group totals for all passes and buckets are
 * atomically added to the single global `bucket_count_buffer`.
 *
 * The result is a complete global histogram in `bucket_count_buffer` (size `RADIX_BASE * 4`),
 * ready to be scanned to determine offsets for all passes of the sort.
 */
[numthreads(THREADS_PER_GROUP, 1, 1)]
void CountRadixBuckets(uint group_thread_id : SV_GroupThreadID, uint group_id : SV_GroupID)
{
#if defined(USE_INDIRECT_DISPATCH)
    const uint sort_count = get_sort_count();
#endif

    [unroll(GROUP_SHARED_SIZE / THREADS_PER_GROUP)]
    for (uint i = group_thread_id; i < GROUP_SHARED_SIZE; i += THREADS_PER_GROUP)
        group_shared[i] = 0u;

    GroupMemoryBarrierWithGroupSync();

    const uint group_shared_offset = (group_thread_id >> 6u) * RADIX_BASE; // 0, 0, ... , 0, 256, 256, ... , 256
    const uint group_item_end = min(sort_count, (group_id + 1u) * ITEMS_PER_GROUP);

    for (uint j = group_thread_id + group_id * ITEMS_PER_GROUP; j < group_item_end; j += THREADS_PER_GROUP)
    {
        const uint sorting_key = from_original_key_to_sorting_key(key_in_buffer[j]);
        InterlockedAdd(group_shared[get_radix_digit(sorting_key,  0u) + group_shared_offset].x, 1u);
        InterlockedAdd(group_shared[get_radix_digit(sorting_key,  8u) + group_shared_offset].y, 1u);
        InterlockedAdd(group_shared[get_radix_digit(sorting_key, 16u) + group_shared_offset].z, 1u);
        InterlockedAdd(group_shared[get_radix_digit(sorting_key, 24u) + group_shared_offset].w, 1u);
    }

    GroupMemoryBarrierWithGroupSync();

    [unroll(RADIX_BASE / THREADS_PER_GROUP)]
    for (uint k = group_thread_id; k < RADIX_BASE; k += THREADS_PER_GROUP)
    {
        InterlockedAdd(bucket_count_buffer[k                  ], group_shared[k].x + group_shared[k + RADIX_BASE].x);
        InterlockedAdd(bucket_count_buffer[k + RADIX_BASE * 1u], group_shared[k].y + group_shared[k + RADIX_BASE].y);
        InterlockedAdd(bucket_count_buffer[k + RADIX_BASE * 2u], group_shared[k].z + group_shared[k + RADIX_BASE].z);
        InterlockedAdd(bucket_count_buffer[k + RADIX_BASE * 3u], group_shared[k].w + group_shared[k + RADIX_BASE].w);
    }
}